# config.yml - Centralized configuration for wp-chat
# Cross-Encoder Reranker and Hybrid Search parameters

# Hybrid Search Configuration
hybrid:
  alpha: 0.6              # Dense weight (BM25 weight = 1 - alpha)
  k_bm25: 100             # BM25 retrieval count
  k_dense: 100            # Dense retrieval count

# MMR Diversification
mmr:
  lambda: 0.7             # Diversity vs relevance tradeoff (0.0-1.0)
  topn: 30               # Number of candidates for diversification

# Cross-Encoder Reranker
reranker:
  # Model selection strategy
  auto_detect: true              # Automatically detect CPU/GPU and select model
  fallback_to_cpu: true          # Fallback to CPU model if GPU fails

  # Available models
  models:
    cpu: "cross-encoder/ms-marco-MiniLM-L-6-v2"      # Lightweight CPU model
    gpu: "cross-encoder/ms-marco-MiniLM-L-12-v2"     # More accurate GPU model
    fallback: "cross-encoder/ms-marco-MiniLM-L-6-v2" # Fallback model

  # Performance settings
  batch_size: 16         # Batch size for efficient scoring
  timeout_sec: 5.0       # Timeout threshold for fallback
  device: null           # Device override (null = auto, "cpu", "cuda", "mps")

  # Composite scoring experiments
  composite_scoring:
    enabled: true        # Enable composite scoring experiments
    alpha: 0.8          # CE weight (hybrid weight = 1 - alpha)
    strategies:
      - name: "ce_only"     # Pure Cross-Encoder scoring
        alpha: 1.0
      - name: "hybrid_only" # Pure Hybrid scoring
        alpha: 0.0
      - name: "balanced"    # Balanced combination
        alpha: 0.5
      - name: "ce_heavy"    # CE-heavy combination
        alpha: 0.8

# API Configuration
api:
  topk_default: 5        # Default number of results
  topk_max: 10          # Maximum number of results
  snippet_length: 400   # Maximum snippet length for highlighting

  # Rate limiting configuration
  rate_limit:
    enabled: true       # Enable rate limiting
    max_requests: 100   # Max requests per window
    window_seconds: 3600 # Time window (1 hour)
    burst_limit: 20     # Burst requests allowed

  # Caching configuration
  cache:
    enabled: true       # Enable caching
    search_ttl: 1800   # Search results TTL (30 minutes)
    embedding_ttl: 7200 # Embeddings TTL (2 hours)
    max_size_mb: 100   # Maximum cache size

  # SLO monitoring configuration
  slo:
    enabled: true      # Enable SLO monitoring
    targets:
      search:
        p95_latency_ms: 800    # Target p95 latency
        success_rate: 0.995   # Target success rate (99.5%)
        window_minutes: 5     # Monitoring window
        alert_threshold: 0.9  # Alert threshold
      ask:
        p95_latency_ms: 1000   # Target p95 latency
        success_rate: 0.99     # Target success rate (99%)
        window_minutes: 5      # Monitoring window
        alert_threshold: 0.9   # Alert threshold
      generate:
        ttft_ms: 1200         # Time to first token
        p95_latency_ms: 6000   # Target p95 latency
        success_rate: 0.98     # Target success rate (98%)
        window_minutes: 5      # Monitoring window
        alert_threshold: 0.9   # Alert threshold

  # Canary deployment configuration
  canary:
    enabled: false     # Enable canary deployment
    rollout_percentage: 0.0  # Initial rollout percentage (0.0 to 1.0)
    user_seed: "canary_seed_2024"  # Seed for consistent user assignment
    auto_rollback_threshold: 0.05  # Auto-rollback if error rate exceeds 5%
    latency_threshold_ms: 2000     # Auto-rollback if latency exceeds 2s
    min_requests_for_rollback: 100 # Minimum requests before considering rollback

  # Backup and restore configuration
  backup:
    enabled: true      # Enable backup system
    schedule:
      full_backup_days: 7      # Full backup every 7 days
      incremental_hours: 24     # Incremental backup every 24 hours
      retention_days: 30        # Keep backups for 30 days
      max_backups: 10           # Maximum number of backups to keep
    paths:
      index: "data/index/"      # Index files to backup
      cache: "logs/cache/"      # Cache files to backup
      config: "config.yml"      # Configuration file to backup
      logs: "logs/"             # Log files to backup
    compression: true           # Enable compression
    verification: true          # Enable backup verification
    auto_cleanup: true          # Enable automatic cleanup

# LLM Configuration (MVP4)
llm:
  provider: openai
  alias: default-mini  # Switch models by changing alias
  timeout_sec: 30
  stream: true

models:
  default-mini:
    name: gpt-4o-mini
    temperature: 0.2
    max_tokens: 700
    description: "Cost-efficient, fast, good Japanese support"
  alt-1:
    name: gpt-4.1-mini
    temperature: 0.2
    max_tokens: 700
    description: "Enhanced reading comprehension (future)"
  alt-2:
    name: o4-mini
    temperature: 0.2
    max_tokens: 700
    description: "Reasoning-optimized, high-score (future)"

generation:
  context_max_tokens: 3500
  chunk_max_tokens: 1000
  max_chunks: 5
  citation_style: "bracketed"  # [[1]], [[2]]

# Evaluation Configuration
eval:
  default_k: 5          # Default evaluation k
  queries_file: "eval/queries.jsonl"
