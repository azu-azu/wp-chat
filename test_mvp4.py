#!/usr/bin/env python3
# test_mvp4.py - Quick test of MVP4 RAG generation
import os
import sys
import asyncio

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_imports():
    """Test that all modules can be imported"""
    print("ğŸ§ª Testing imports...")

    try:
        from src.generation import generation_pipeline
        print("âœ… generation.py imported")

        from src.prompts import build_messages, validate_citations
        print("âœ… prompts.py imported")

        from src.openai_client import openai_client
        print("âœ… openai_client.py imported")

        from src.config import get_config_value
        print("âœ… config.py imported")

        return True
    except Exception as e:
        print(f"âŒ Import error: {e}")
        return False

def test_config():
    """Test configuration loading"""
    print("\nğŸ”§ Testing configuration...")

    try:
        from src.config import get_config_value

        # Test LLM config
        provider = get_config_value("llm.provider")
        alias = get_config_value("llm.alias")
        model_name = get_config_value(f"models.{alias}.name")

        print(f"âœ… Provider: {provider}")
        print(f"âœ… Alias: {alias}")
        print(f"âœ… Model: {model_name}")

        # Test generation config
        context_max = get_config_value("generation.context_max_tokens")
        citation_style = get_config_value("generation.citation_style")

        print(f"âœ… Context max tokens: {context_max}")
        print(f"âœ… Citation style: {citation_style}")

        return True
    except Exception as e:
        print(f"âŒ Config error: {e}")
        return False

def test_prompt_building():
    """Test prompt building"""
    print("\nğŸ’¬ Testing prompt building...")

    try:
        from src.prompts import build_messages, validate_citations

        # Mock documents
        docs = [
            {
                "title": "Test Document 1",
                "url": "https://example.com/doc1",
                "snippet": "This is a test document with relevant information."
            },
            {
                "title": "Test Document 2",
                "url": "https://example.com/doc2",
                "snippet": "Another test document with additional context."
            }
        ]

        # Build messages
        messages = build_messages("What is the main topic?", docs)

        print(f"âœ… Messages built: {len(messages)} messages")
        print(f"âœ… System prompt length: {len(messages[0]['content'])} chars")
        print(f"âœ… User prompt length: {len(messages[1]['content'])} chars")

        # Test citation validation
        test_text = "This is a test with citations [[1]] and [[2]]."
        validation = validate_citations(test_text, len(docs))

        print(f"âœ… Citations found: {validation['citations']}")
        print(f"âœ… Citation count: {validation['citation_count']}")
        print(f"âœ… Is valid: {validation['is_valid']}")

        return True
    except Exception as e:
        print(f"âŒ Prompt building error: {e}")
        return False

def test_context_composition():
    """Test context composition"""
    print("\nğŸ“ Testing context composition...")

    try:
        from src.generation import generation_pipeline

        # Mock documents
        docs = [
            {
                "title": "Document 1",
                "url": "https://example.com/doc1",
                "snippet": "This is a long document with lots of content that should be truncated if it exceeds the token limit.",
                "post_id": "1",
                "chunk_id": "1",
                "hybrid_score": 0.95
            },
            {
                "title": "Document 2",
                "url": "https://example.com/doc2",
                "snippet": "Another document with different content.",
                "post_id": "2",
                "chunk_id": "1",
                "hybrid_score": 0.87
            }
        ]

        # Process context
        processed_docs, metadata = generation_pipeline.process_retrieval_results(docs)

        print(f"âœ… Processed docs: {len(processed_docs)}")
        print(f"âœ… Total tokens: {metadata['total_tokens']}")
        print(f"âœ… Chunks used: {metadata['chunks_used']}")
        print(f"âœ… Original chunks: {metadata['original_chunks']}")

        return True
    except Exception as e:
        print(f"âŒ Context composition error: {e}")
        return False

def test_openai_client_init():
    """Test OpenAI client initialization"""
    print("\nğŸ¤– Testing OpenAI client...")

    try:
        from src.openai_client import openai_client

        # Test model info
        model_info = openai_client.get_model_info()

        print(f"âœ… Model alias: {model_info['alias']}")
        print(f"âœ… Model name: {model_info['name']}")
        print(f"âœ… Temperature: {model_info['temperature']}")
        print(f"âœ… Max tokens: {model_info['max_tokens']}")

        # Test if API key is set
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            print(f"âœ… API key is set (length: {len(api_key)})")
        else:
            print("âš ï¸  API key not set - set OPENAI_API_KEY environment variable")

        return True
    except Exception as e:
        print(f"âŒ OpenAI client error: {e}")
        return False

def main():
    """Run all tests"""
    print("ğŸš€ MVP4 RAG Generation - Quick Test")
    print("=" * 50)

    tests = [
        test_imports,
        test_config,
        test_prompt_building,
        test_context_composition,
        test_openai_client_init
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        if test():
            passed += 1
        print()

    print("=" * 50)
    print(f"ğŸ“Š Results: {passed}/{total} tests passed")

    if passed == total:
        print("ğŸ‰ All tests passed! MVP4 implementation looks good.")
        print("\nğŸ“‹ Next steps:")
        print("1. Set OPENAI_API_KEY environment variable")
        print("2. Run: python src/generate_cli.py --health")
        print("3. Test with: python src/generate_cli.py --interactive")
        return 0
    else:
        print("âŒ Some tests failed. Check the errors above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
